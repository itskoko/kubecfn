---
AWSTemplateFormatVersion: '2010-09-09'
Parameters:
  DomainName:
    Type: String

  ClusterState:
    Type: String
    Description: |
      etcd cluster state: If this option is set to existing, etcd
      will attempt to join the existing cluster. If the wrong value is set, etcd
      will attempt to start but fail safely.
    Default: existing

  KeyName:
    Description: Existing EC2 KeyPair for SSH access.
    Type: AWS::EC2::KeyPair::KeyName
    ConstraintDescription: must be the name of an existing EC2 KeyPair.
    Default: cfn-kubernetes

  DomainName:
    Type: String

  ParentZoneID:
    Type: AWS::Route53::HostedZone::Id

  ControllerSecurityGroup:
    Description: Subdomain for controller ELB, relative to DomainName
    Type: AWS::EC2::SecurityGroup::Id

  ControllerELBSecurityGroup:
    Type: AWS::EC2::SecurityGroup::Id

  ControllerSubdomain:
    Description: Subdomain for controller ELB, relative to DomainName
    Type: String

  ControllerSubdomainInt:
    Description: Subdomain for controller ELB, relative to DomainName
    Type: String

  ControllerInstanceType:
    Description: EC2 instance type for controller nodes.
    Type: String
    Default: m4.medium

  WorkerInstanceType:
    Description: EC2 instance type for controller nodes.
    Type: String
    Default: c5.xlarge

  ControllerPoolSize:
    Type: Number
    Default: 3

  ControllerVolumeSize:
    Type: Number
    Default: 50

  ControllerVolumeType:
    Type: String
    Default: gp2

  KubeVersion:
    Type: String
    Default: v1.11.2

  KubeadmFeatureGates:
    Type: CommaDelimitedList
    Default: ""

  WorkerSecurityGroup:
    Type: AWS::EC2::SecurityGroup::Id

  WorkerRoleName:
    Type: String

  WorkerFeatureGates:
    Type: String
    Default: ""

  WorkerCPUManagerPolicy:
    Type: String
    Default: "none"

  KubeadmVersion:
    Type: String
    Default: v1.11.2

  KubeadmURLRoot:
    Type: String
    Description: No trailing /
    Default: https://storage.googleapis.com/kubernetes-release/release

  KubeadmURLPath:
    Type: String
    Description: No leading /
    Default: bin/linux/amd64/kubeadm

  CriCtlVersion:
    Type: String
    Description: crictl version
    Default: v1.11.1

  VPCID:
    Description: Existing VPC with attached internet gateway to use for this cluster.
    Type: AWS::EC2::VPC::Id

  VPCID:
    Description: Existing VPC with attached internet gateway to use for this cluster.
    Type: AWS::EC2::VPC::Id

  InternetGateway:
    Description: The InternetGateway attached to the VPC
    Type: String

  PublicSubnetA:
    Type: AWS::EC2::Subnet::Id

  PublicSubnetB:
    Type: AWS::EC2::Subnet::Id

  PublicSubnetC:
    Type: AWS::EC2::Subnet::Id

  PrivateSubnetA:
    Type: AWS::EC2::Subnet::Id

  PrivateSubnetB:
    Type: AWS::EC2::Subnet::Id

  PrivateSubnetC:
    Type: AWS::EC2::Subnet::Id

  BastionSecurityGroup:
    Type: AWS::EC2::SecurityGroup::Id

  ControllerRoleName:
    Type: String

  AutoScalingDNSUpdateLambdaRoleARN:
    Type: String

  assetBucket:
    Type: String

Mappings:
  Assets:
    etcd:
      # FIXME ExecStartPre=/bin/chown might not be needed since we bumped
      # ignition version.
      unit: |
        [Unit]
        Requires=coreos-metadata.service
        After=coreos-metadata.service

        [Service]
        EnvironmentFile=/run/metadata/coreos
        EnvironmentFile=/etc/etcd.env
        Environment="RKT_RUN_ARGS=--volume etcd-ssl,kind=host,source=/etc/ssl/etcd \
          --mount volume=etcd-ssl,target=/etc/ssl/etcd"
        # member-add fails if its run again before etcd comes up, so we ignore
        # the error for now. A failure to add a peer will result in etcd
        # failing and we can monitor that.
        ExecStartPre=-/etc/etcd-member-add "${COREOS_EC2_HOSTNAME}"
        ExecStart=
        ExecStart=/usr/lib/coreos/etcd-wrapper $ETCD_OPTS \
          --name="${COREOS_EC2_HOSTNAME}" \
          --listen-peer-urls="https://${COREOS_EC2_IPV4_LOCAL}:2380" \
          --listen-client-urls="https://0.0.0.0:2379" \
          --initial-advertise-peer-urls="https://${COREOS_EC2_HOSTNAME}:2380" \
          --advertise-client-urls="https://${COREOS_EC2_HOSTNAME}:2379" \
          --trusted-ca-file=/etc/ssl/etcd/ca.crt \
          --cert-file=/etc/ssl/etcd/server.crt \
          --key-file=/etc/ssl/etcd/server.key \
          --peer-cert-file=/etc/ssl/etcd/peer.crt \
          --peer-key-file=/etc/ssl/etcd/peer.key \
          --peer-trusted-ca-file=/etc/ssl/etcd/ca.crt \
          --client-cert-auth=true \
          --peer-client-cert-auth=true
        ExecStop=
        ExecStop=/etc/etcd-member-remove "${COREOS_EC2_HOSTNAME}"
        ExecStop=-/usr/bin/rkt stop --uuid-file=/var/lib/coreos/etcd-member-wrapper.uuid
      etcdctlWrapper: |
        #!/bin/bash
        . /etc/etcd.env

        etcdctl -D ${ETCD_DISCOVERY_SRV} \
          --ca-file=/etc/ssl/etcd/ca.crt \
          --cert-file=/etc/ssl/etcd/peer.crt \
          --key-file=/etc/ssl/etcd/peer.key \
          --no-sync "$@"

      etcdMemberAdd: |
        #!/bin/bash
        set -euo pipefail
        . /run/metadata/coreos
        if [[ "$ETCD_INITIAL_CLUSTER_STATE" == "new" ]]; then
          echo "New cluster, exiting"
          exit 0
        else
          echo "Adding ourself to cluster"
          /etc/etcdctl-wrapper member add "$COREOS_EC2_HOSTNAME" "https://$COREOS_EC2_HOSTNAME:2380"
        fi

      etcdMemberRemove: |
        #!/bin/bash
        set -euo pipefail
        hostname=$1
        ID=$(/etc/etcdctl-wrapper member list | awk -F: "/name=$hostname / { print \$1 }")
        /etc/etcdctl-wrapper member remove "$ID"

    kubelet:
      unit: |
        [Unit]
        Description=Kubernetes Kubelet Server
        Documentation=https://github.com/kubernetes/kubernetes
        Requires=coreos-metadata.service
        After=coreos-metadata.service

        [Service]
        EnvironmentFile=/run/metadata/coreos
        EnvironmentFile=/etc/kubernetes.env
        Environment="RKT_RUN_ARGS=--uuid-file-save=/var/run/kubelet-pod.uuid \
          --volume dns,kind=host,source=/etc/resolv.conf \
          --volume cni-opt,kind=host,source=/opt/cni \
          --volume cni-etc,kind=host,source=/etc/cni \
          --mount volume=dns,target=/etc/resolv.conf \
          --mount volume=cni-opt,target=/opt/cni \
          --mount volume=cni-etc,target=/etc/cni"
        ExecStartPre=/bin/mkdir -p /opt/cni /etc/cni
        ExecStartPre=-/usr/bin/rkt rm --uuid-file=/var/run/kubelet-pod.uui
        ExecStart=/usr/lib/coreos/kubelet-wrapper \
          --kubeconfig=/etc/kubernetes/admin.conf \
          --pod-manifest-path=/etc/kubernetes/manifests \
          --cloud-provider=aws \
          --cloud-config=/etc/kubernetes/cloud-config \
          --network-plugin=cni \
          --pod-cidr=10.244.0.0/16 \
          --cluster-dns=10.96.0.10 \
          --node-ip=${COREOS_EC2_IPV4_LOCAL} \
          --cluster-domain=${KUBELET_CLUSTER_DOMAIN} \
          --allow-privileged
        ExecStop=-/usr/bin/rkt stop --uuid-file=/var/run/kubelet-pod.uuid
        [Install]
        WantedBy=multi-user.target
    kubeadm:
      unit: |
        [Unit]
        After=etcd-member.service network-online.target
        Description=Kubeadm init
        Documentation=https://github.com/kubernetes/kubernetes

        [Service]
        Type=oneshot
        EnvironmentFile=/run/metadata/coreos
        EnvironmentFile=/etc/kubernetes.env
        Environment="PATH=/bin:/sbin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/bin"
        ExecStartPre=/bin/sh -c 'grep nodeName: /etc/kubernetes/controller.yaml \
          && exit 0; echo "nodeName: $COREOS_EC2_HOSTNAME" \
          >> /etc/kubernetes/controller.yaml'
        ExecStartPre=/bin/sh -c 'test -f /opt/bin/crictl && exit 0; \
          curl -sfL "https://github.com/kubernetes-incubator/cri-tools/releases/download/${CRICTL_VERSION}/crictl-${CRICTL_VERSION}-linux-amd64.tar.gz" \
            | tar -C /opt/bin/ -xzvf -'
        ExecStart=/bin/sh -c 'while ! /opt/bin/kubeadm init --config /etc/kubernetes/controller.yaml \
          --ignore-preflight-errors=KubeletVersion,Port-10250; do sleep 1; done'
        ExecStartPost=/etc/etcd-signal-health
        [Install]
        WantedBy=multi-user.target

  # Generate with:
  # curl -L https://coreos.com/dist/aws/aws-stable.json \
  #   | jq 'to_entries|map(select(.key != "release_info"))|from_entries' \
  #   | json2yaml | sed 's/^/    /'
  RegionToImageMap:
    ap-northeast-1:
      hvm: ami-55e41e2a
      pv: ami-69e51f16
    ap-northeast-2:
      hvm: ami-c09338ae
      pv: ami-aa903bc4
    ap-south-1:
      hvm: ami-84406eeb
      pv: ami-ae5e70c1
    ap-southeast-1:
      hvm: ami-86bf81fa
      pv: ami-19c1ff65
    ap-southeast-2:
      hvm: ami-f6e53794
      pv: ami-b7e537d5
    ca-central-1:
      hvm: ami-6df57609
      pv: ami-06fa7962
    cn-north-1:
      hvm: ami-555a8438
      pv: ami-fc5c8291
    cn-northwest-1:
      hvm: ami-06a0b464
    eu-central-1:
      hvm: ami-4a83b6a1
      pv: ami-8680b56d
    eu-west-1:
      hvm: ami-c70005be
      pv: ami-2572775c
    eu-west-2:
      hvm: ami-177a9670
      pv: ami-db7a96bc
    eu-west-3:
      hvm: ami-d240f1af
    sa-east-1:
      hvm: ami-a82079c4
      pv: ami-9e267ff2
    us-east-1:
      hvm: ami-a32d46dc
      pv: ami-6b3e5514
    us-east-2:
      hvm: ami-36497653
      pv: ami-e4487781
    us-gov-west-1:
      hvm: ami-8ccc5ded
      pv: ami-54cf5e35
    us-west-1:
      hvm: ami-6e647e0e
      pv: ami-161a0076
    us-west-2:
      hvm: ami-4296ec3a
      pv: ami-0e90ea76

Conditions:
  isExisting: !Equals [ {"Ref": "ClusterState"}, "existing" ]
  hasNoParentZone: !Equals [ {"Ref": "ParentZoneID"}, "" ]
  hasParentZone: !Not [ { "Condition": "hasNoParentZone" } ]

Resources:
  BastionHost:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: !FindInMap [ RegionToImageMap, !Ref "AWS::Region", hvm ]
      InstanceType: t2.micro
      SubnetId: !Ref PublicSubnetA
      SecurityGroupIds: [ !Ref BastionSecurityGroup ]
      KeyName: !Ref KeyName

  BastionRecordSet:
    Type: "AWS::Route53::RecordSet"
    Properties:
      HostedZoneId: !Ref HostedZone
      Name: !Join [ ".", [ "bastion", !Ref DomainName, "" ] ]
      Type: A
      TTL: 60
      ResourceRecords:
        - !GetAtt [ "BastionHost", "PublicIp" ]

  AutoScalingNotificationTopic:
    Type: "AWS::SNS::Topic"
    Properties:
      DisplayName: "Controller AutoScaling Events"
      Subscription:
        - Protocol: lambda
          Endpoint: !GetAtt [ AutoScalingDNSUpdateLambda, Arn ]

  ControllerAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      VPCZoneIdentifier:
        - !Ref PrivateSubnetA
        - !Ref PrivateSubnetB
        - !Ref PrivateSubnetC
      LaunchConfigurationName:
        Ref: ControllerLaunchConfiguration
      DesiredCapacity: !Ref ControllerPoolSize
      MaxSize: !Ref ControllerPoolSize
      MinSize: !Ref ControllerPoolSize
      LoadBalancerNames:
        - !Ref ControllerELB
        - !Ref ControllerELBInt
      NotificationConfigurations:
        - NotificationTypes:
            - "autoscaling:EC2_INSTANCE_LAUNCH"
            - "autoscaling:EC2_INSTANCE_TERMINATE"
          TopicARN: !Ref AutoScalingNotificationTopic
      Tags:
      - Key: StackName
        PropagateAtLaunch: true
        Value: !Ref AWS::StackName
      - Key: KubernetesCluster
        PropagateAtLaunch: true
        Value: !Ref DomainName
    UpdatePolicy:
      AutoScalingRollingUpdate:
        MaxBatchSize: 1
        MinInstancesInService: 2
        WaitOnResourceSignals: true
        PauseTime: PT1H

  AutoScalingDNSUpdateLambda:
    Type: "AWS::Lambda::Function"
    Properties:
      Role: !Ref AutoScalingDNSUpdateLambdaRoleARN
      Runtime: python3.6
      Handler: index.lambda_handler
      Code:
        ZipFile: !Sub |
          import boto3
          import json
          domain = "${DomainName}"
          hostedZoneID= "${HostedZone}"
          prefix = 'controller-'
          ec2 = boto3.client('ec2')
          route53 = boto3.client('route53')
          autoscaling = boto3.client('autoscaling')

          def lambda_handler(event, context):
            print('Processing event: {}'.format(event))
            record = event['Records'][0]
            message = json.loads(record['Sns']['Message'])

            asgis = autoscaling.describe_auto_scaling_groups(
              AutoScalingGroupNames = [ message['AutoScalingGroupName'] ],
              MaxRecords = 1
            )['AutoScalingGroups'][0]['Instances']
            print('Found instance ids: {}'.format(asgis))

            reservations = ec2.describe_instances(
              InstanceIds = list(map(lambda i: i['InstanceId'], asgis))
            )['Reservations']

            instances = [ y for x in list(map(lambda r: r['Instances'],
              reservations)) for y in x ]
            print('Found instances: {}'.format(instances))

            changes = [{
              'Action': 'UPSERT',
              'ResourceRecordSet': {
                'Type': 'SRV',
                'Name': '_etcd-server-ssl._tcp.' + domain,
                'TTL': 60,
                'ResourceRecords': list(map(
                  lambda i: { 'Value': '0 0 2380 ' + i['PrivateDnsName'] },
                  instances
                ))
              }
            }, {
              'Action': 'UPSERT',
              'ResourceRecordSet': {
                'Type': 'SRV',
                'Name': '_etcd-client-ssl._tcp.' + domain,
                'TTL': 60,
                'ResourceRecords': list(map(
                  lambda i: { 'Value': '0 0 2379 ' + i['PrivateDnsName'] },
                  instances
                ))
              }
            }]

            print('Applying these Changes: {}'.format(changes))
            route53.change_resource_record_sets(HostedZoneId=hostedZoneID,
              ChangeBatch={ 'Changes': changes }
            )


  LambdaInvokePermission:
    Type: "AWS::Lambda::Permission"
    Properties:
      Action: "lambda:InvokeFunction"
      Principal: sns.amazonaws.com
      SourceArn: !Ref AutoScalingNotificationTopic
      FunctionName: !GetAtt [ AutoScalingDNSUpdateLambda, Arn ]

  ControllerInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Path: /
      Roles:
        - !Ref ControllerRoleName

  ControllerLaunchConfiguration:
    Type: AWS::AutoScaling::LaunchConfiguration
    Properties:
      KeyName: !Ref KeyName
      ImageId: !FindInMap [ RegionToImageMap, !Ref "AWS::Region", hvm ]
      InstanceType: !Ref ControllerInstanceType
      IamInstanceProfile: !GetAtt [ ControllerInstanceProfile, Arn ]
      SecurityGroups:
        - !Ref ControllerSecurityGroup
      BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: !Ref ControllerVolumeSize
            VolumeType: !Ref ControllerVolumeType
      UserData:
        Fn::Base64:
          Fn::Sub:
            - |
              {
                "ignition": {
                  "version": "2.1.0",
                  "config": {}
                },
                "storage": {
                  "files": [{
                    "filesystem": "root",
                    "path": "/etc/ssl/etcd/server.crt",
                    "user": { "name": "etcd" },
                    "mode": 420,
                    "contents": { "source": "s3://${assetBucket}/${domain}/etcd/server.pem" }
                  }, {
                    "filesystem": "root",
                    "path": "/etc/ssl/etcd/server.key",
                    "user": { "name": "etcd" },
                    "mode": 384,
                    "contents": { "source": "s3://${assetBucket}/${domain}/etcd/server-key.pem" }
                  }, {
                    "filesystem": "root",
                    "path": "/etc/ssl/etcd/peer.crt",
                    "user": { "name": "etcd" },
                    "mode": 420,
                    "contents": { "source": "s3://${assetBucket}/${domain}/etcd/peer.pem" }
                  }, {
                    "filesystem": "root",
                    "path": "/etc/ssl/etcd/peer.key",
                    "user": { "name": "etcd" },
                    "mode": 384,
                    "contents": { "source": "s3://${assetBucket}/${domain}/etcd/peer-key.pem" }
                  }, {
                    "filesystem": "root",
                    "path": "/etc/ssl/etcd/ca.crt",
                    "user": { "name": "etcd" },
                    "mode": 420,
                    "contents": { "source": "s3://${assetBucket}/${domain}/etcd/ca.pem" }
                  }, {
                    "filesystem": "root",
                    "path": "/etc/kubernetes/pki/ca.crt",
                    "mode": 420,
                    "contents": { "source": "s3://${assetBucket}/${domain}/kubeadm/ca.crt" }
                  }, {
                    "filesystem": "root",
                    "path": "/etc/kubernetes/pki/ca.key",
                    "mode": 384,
                    "contents": { "source": "s3://${assetBucket}/${domain}/kubeadm/ca.key" }
                  }, {
                    "filesystem": "root",
                    "path": "/etc/kubernetes/pki/sa.pub",
                    "mode": 420,
                    "contents": { "source": "s3://${assetBucket}/${domain}/kubeadm/sa.pub" }
                  }, {
                    "filesystem": "root",
                    "path": "/etc/kubernetes/pki/sa.key",
                    "mode": 384,
                    "contents": { "source": "s3://${assetBucket}/${domain}/kubeadm/sa.key" }
                  }, {
                    "filesystem": "root",
                    "path": "/etc/kubernetes/pki/front-proxy-ca.crt",
                    "mode": 420,
                    "contents": { "source": "s3://${assetBucket}/${domain}/kubeadm/front-proxy-ca.crt" }
                  }, {
                    "filesystem": "root",
                    "path": "/etc/kubernetes/pki/front-proxy-ca.key",
                    "mode": 384,
                    "contents": { "source": "s3://${assetBucket}/${domain}/kubeadm/front-proxy-ca.key" }
                  }, {
                   "filesystem": "root",
                    "path": "/etc/kubernetes/cloud-config",
                    "mode": 420,
                    "contents": { "source": "data:;base64,${cloudProviderConfig}" }
                  },
                  {
                    "filesystem": "root",
                    "path": "/etc/kubernetes/controller.yaml",
                    "mode": 420,
                    "contents": { "source": "data:;base64,${controllerConfig}" }
                  },
                  {
                    "filesystem": "root",
                    "path": "/etc/etcd.env",
                     "mode": 420,
                    "contents": { "source": "data:;base64,${etcdEnv}" }
                  }, {
                    "filesystem": "root",
                    "path": "/etc/etcdctl-wrapper",
                    "mode": 493,
                    "contents": { "source": "data:;base64,${etcdctlWrapper}" }
                  }, {
                    "filesystem": "root",
                    "path": "/etc/etcd-member-remove",
                    "mode": 493,
                    "contents": { "source": "data:;base64,${etcdMemberRemove}" }
                  }, {
                    "filesystem": "root",
                    "path": "/etc/etcd-member-add",
                    "mode": 493,
                    "contents": { "source": "data:;base64,${etcdMemberAdd}" }
                  }, {
                    "filesystem": "root",
                    "path": "/etc/etcd-signal-health",
                    "mode": 493,
                    "contents": { "source": "data:;base64,${etcdSignalHealth}" }
                  }, {
                    "filesystem": "root",
                    "path": "/opt/bin/kubeadm",
                    "mode": 493,
                    "contents": { "source": "${kubeadmURL}" }
                  }, {
                    "filesystem": "root",
                    "path": "/etc/kubernetes.env",
                     "mode": 420,
                    "contents": { "source": "data:;base64,${kubernetesEnv}" }
                  }]
                },
                "systemd": {
                  "units": [{
                    "name": "etcd-member.service",
                    "enable": true,
                    "dropins": [{
                      "name": "20-etcd-member.conf",
                      "contents": "${etcdUnit}"
                    }]
                  }, {
                    "name": "kubelet.service",
                    "enable": false,
                    "contents": "${kubeletUnit}"
                  }, {
                    "name": "kubeadm.service",
                    "enable": true,
                    "contents": "${kubeadmUnit}"
                  }, {
                    "name": "update-engine.service",
                    "mask": true
                  }, {
                    "name": "locksmithd.service",
                    "mask": true
                  }]
                },
                "networkd": {},
                "passwd": {}
              }
            - etcdUnit: !Join
              - "\\n"
              - !Split
                - "\n"
                - !Join
                  - "\\\""
                  - !Split
                    - "\""
                    - !Join
                      - "\\\\"
                      - !Split
                        - "\\"
                        - !FindInMap [ Assets, etcd, unit ]
              kubeletUnit: !Join
              - "\\n"
              - !Split
                - "\n"
                - !Join
                  - "\\\""
                  - !Split
                    - "\""
                    - !Join
                      - "\\\\"
                      - !Split
                        - "\\"
                        - !FindInMap [ Assets, kubelet, unit ]
              kubeadmUnit: !Join
              - "\\n"
              - !Split
                - "\n"
                - !Join
                  - "\\\""
                  - !Split
                    - "\""
                    - !Join
                      - "\\\\"
                      - !Split
                        - "\\"
                        - !FindInMap [ Assets, kubeadm, unit ]
              # Scripts
              etcdctlWrapper:
                Fn::Base64: !FindInMap [ Assets, etcd, etcdctlWrapper ]
              etcdMemberRemove:
                Fn::Base64: !FindInMap [ Assets, etcd, etcdMemberRemove ]
              etcdMemberAdd:
                Fn::Base64: !FindInMap [ Assets, etcd, etcdMemberAdd ]
              etcdSignalHealth:
                Fn::Base64: !Sub
                  - |
                    #!/bin/bash
                    set -euo pipefail
                    echo "Wait for etcd to join cluster"
                    while ! /etc/etcdctl-wrapper member list | grep "$(hostname)"; do sleep 1; done
                    echo "Wait for cluster-health"
                    while true; do
                      sleep 1
                      /etc/etcdctl-wrapper cluster-health | tee /tmp/cluster-health.txt
                      if [[ "${!PIPESTATUS[0]}" -ne 0 ]]; then
                        echo " - no quorum, retrying"
                        continue
                      fi
                      if [[ "$(cat /tmp/cluster-health.txt | wc -l)" -ne "$((${ControllerPoolSize}+1))" ]]; then
                        echo "- unexpected number of peers"
                        continue
                      fi
                      # continue if all lines contain 'is healthy'
                      if ! grep -v "is healthy" /tmp/cluster-health.txt; then
                        break
                      fi
                      echo " - unhealthy members found, retrying"
                    done
                    echo "Signaling success"
                    docker run --rm --net=host rochacon/cfn-bootstrap cfn-signal \
                      --resource ControllerAutoScalingGroup \
                      --stack ${StackName} \
                      --region ${Region} || true # Ignore if signaling failed
                  - StackName: !Ref AWS::StackName
                    Region: !Ref AWS::Region
                    ControllerPoolSize: !Ref ControllerPoolSize
              # Environment files
              etcdEnv:
                Fn::Base64:
                  Fn::Sub:
                    - |
                      ETCD_DISCOVERY_SRV=${DomainName}
                      ETCD_INITIAL_CLUSTER_TOKEN=${DomainName}
                      ETCD_INITIAL_CLUSTER_STATE=${ClusterState}
                    - DomainName: !Ref DomainName
                      ClusterState: !Ref ClusterState
              kubernetesEnv:
                Fn::Base64:
                  Fn::Sub:
                    - |
                      KUBELET_IMAGE_TAG=${KubeVersion}
                      KUBELET_IMAGE_URL=docker://gcr.io/google-containers/hyperkube-amd64
                      KUBELET_CLUSTER_DOMAIN=${DomainName}
                      RKT_GLOBAL_ARGS="--insecure-options=image"
                      CRICTL_VERSION=${CriCtlVersion}
                    - KubeVersion: !Ref KubeVersion
                      DomainName: !Ref DomainName
                      CriCtlVersion: !Ref CriCtlVersion
              cloudProviderConfig:
                Fn::Base64:
                  Fn::Sub:
                    - |
                      [Global]
                      KubernetesClusterTag=${DomainName}
                      KubernetesClusterID=${DomainName}
                    - DomainName: !Ref DomainName
              controllerConfig:
                Fn::Base64:
                  Fn::Sub:
                    - |
                      apiVersion: kubeadm.k8s.io/v1alpha1
                      kind: MasterConfiguration
                      api:
                        controlPlaneEndpoint: ${ControllerSubdomainInt}.${DomainName}
                      etcd:
                        endpoints:
                        - https://localhost:2379
                        caFile: /etc/ssl/etcd/ca.crt
                        certFile: /etc/ssl/etcd/peer.crt
                        keyFile: /etc/ssl/etcd/peer.key
                      kubernetesVersion: ${KubeVersion}
                      cloudProvider: aws
                      kubeProxy:
                        config:
                          featureGates:
                            SupportIPVSProxyMode: true
                          mode: ipvs
                          metricsBindAddress: 0.0.0.0:10249
                      networking:
                        dnsDomain: ${DomainName}
                        podSubnet: 10.244.0.0/16
                      apiServerExtraArgs:
                        apiserver-count: "${ApiserverCount}"
                      apiServerExtraVolumes:
                        - name: ca-certs
                          hostPath: /usr/share/ca-certificates
                          mountPath: /etc/ssl/certs
                      apiServerCertSANs:
                        - ${ControllerSubdomain}.${DomainName}
                        - ${ControllerSubdomainInt}.${DomainName}
                      controllerManagerExtraArgs:
                        flex-volume-plugin-dir: /opt/libexec/kubernetes/kubelet-plugins/volume/exec
                        address: 0.0.0.0
                      controllerManagerExtraVolumes:
                        - name: ca-certs
                          hostPath: /usr/share/ca-certificates
                          mountPath: /etc/ssl/certs
                        - name: flexvolume-dir
                          hostPath: /opt/libexec/kubernetes/kubelet-plugins/volume/exec
                          mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
                      schedulerExtraArgs:
                        address: 0.0.0.0
                      featureGates:
                        ${FeatureGates}
                    - DomainName: !Ref DomainName
                      KubeVersion: !Ref KubeVersion
                      ApiserverCount: !Ref ControllerPoolSize
                      FeatureGates: !Join [ "\n  ", !Ref KubeadmFeatureGates ]

              domain: !Ref DomainName
              kubeadmURL: !Join
                - "/"
                - [!Ref KubeadmURLRoot, !Ref KubeadmVersion, !Ref KubeadmURLPath]

  ControllerELB:
    Type: "AWS::ElasticLoadBalancing::LoadBalancer"
    Properties:
      Subnets:
        - !Ref PublicSubnetA
        - !Ref PublicSubnetB
        - !Ref PublicSubnetC
      CrossZone: true
      SecurityGroups:
        - !Ref ControllerELBSecurityGroup
      ConnectionSettings:
        IdleTimeout: 3600
      HealthCheck:
        HealthyThreshold: 2
        Interval: 30
        Target: SSL:6443
        UnhealthyThreshold: 4
        Timeout: 10
      Listeners:
        - LoadBalancerPort: 6443
          InstancePort: 6443
          Protocol: TCP

  ControllerELBInt:
    Type: "AWS::ElasticLoadBalancing::LoadBalancer"
    Properties:
      Scheme: internal
      Subnets:
        - !Ref PrivateSubnetA
        - !Ref PrivateSubnetB
        - !Ref PrivateSubnetC
      CrossZone: true
      SecurityGroups:
        - !Ref ControllerSecurityGroup
        - !Ref WorkerSecurityGroup
      ConnectionSettings:
        IdleTimeout: 3600
      HealthCheck:
        HealthyThreshold: 2
        Interval: 30
        Target: SSL:6443
        UnhealthyThreshold: 4
        Timeout: 10
      Listeners:
        - LoadBalancerPort: 6443
          InstancePort: 6443
          Protocol: TCP

  HostedZoneDelegation:
    Type: "AWS::Route53::RecordSet"
    Properties:
      HostedZoneId: !Ref ParentZoneID
      Name: !Ref DomainName
      Type: NS
      TTL: 60
      ResourceRecords: !GetAtt [ HostedZone, NameServers ]
    Condition: hasParentZone

  HostedZone:
    Type: "AWS::Route53::HostedZone"
    Properties:
      Name: !Ref DomainName

  RecordSet:
    Type: "AWS::Route53::RecordSet"
    Properties:
      HostedZoneId: !Ref HostedZone
      Name: !Join [ ".", [ !Ref ControllerSubdomain, !Ref DomainName, "" ] ]
      Type: CNAME
      TTL: 60
      ResourceRecords:
        - !GetAtt [ "ControllerELB", "DNSName" ]

  RecordSetInt:
    Type: "AWS::Route53::RecordSet"
    Properties:
      HostedZoneId: !Ref HostedZone
      Name: !Join [ ".", [ !Ref ControllerSubdomainInt, !Ref DomainName, "" ] ]
      Type: CNAME
      TTL: 60
      ResourceRecords:
        - !GetAtt [ "ControllerELBInt", "DNSName" ]

  WorkerPoolDefaultA:
    Type: "AWS::CloudFormation::Stack"
    Properties:
      TemplateURL: !Sub
        - "https://s3.amazonaws.com/${assetBucket}/${DomainName}/templates/worker.yaml"
        - assetBucket: !Ref assetBucket
          DomainName:  !Ref DomainName
      Parameters:
        DomainName:         !Ref DomainName
        FeatureGates:       !Ref WorkerFeatureGates
        CPUManagerPolicy:   !Ref WorkerCPUManagerPolicy
        assetBucket:        !Ref assetBucket
        VPCID:              !Ref VPCID
        PrivateSubnet:      !Ref PrivateSubnetC
        KubeVersion:        !Ref KubeVersion
        InstanceRoleName:   !Ref WorkerRoleName
        SecurityGroup:      !Ref WorkerSecurityGroup
        WorkerInstanceType: !Ref WorkerInstanceType

  WorkerPoolDefaultB:
    Type: "AWS::CloudFormation::Stack"
    Properties:
      TemplateURL: !Sub
        - "https://s3.amazonaws.com/${assetBucket}/${DomainName}/templates/worker.yaml"
        - assetBucket: !Ref assetBucket
          DomainName:  !Ref DomainName
      Parameters:
        DomainName:         !Ref DomainName
        FeatureGates:       !Ref WorkerFeatureGates
        CPUManagerPolicy:   !Ref WorkerCPUManagerPolicy
        assetBucket:        !Ref assetBucket
        VPCID:              !Ref VPCID
        PrivateSubnet:      !Ref PrivateSubnetC
        KubeVersion:        !Ref KubeVersion
        InstanceRoleName:   !Ref WorkerRoleName
        SecurityGroup:      !Ref WorkerSecurityGroup
        WorkerInstanceType: !Ref WorkerInstanceType

  WorkerPoolDefaultC:
    Type: "AWS::CloudFormation::Stack"
    Properties:
      TemplateURL: !Sub
        - "https://s3.amazonaws.com/${assetBucket}/${DomainName}/templates/worker.yaml"
        - assetBucket: !Ref assetBucket
          DomainName:  !Ref DomainName
      Parameters:
        DomainName:         !Ref DomainName
        FeatureGates:       !Ref WorkerFeatureGates
        CPUManagerPolicy:   !Ref WorkerCPUManagerPolicy
        assetBucket:        !Ref assetBucket
        VPCID:              !Ref VPCID
        PrivateSubnet:      !Ref PrivateSubnetC
        KubeVersion:        !Ref KubeVersion
        InstanceRoleName:   !Ref WorkerRoleName
        SecurityGroup:      !Ref WorkerSecurityGroup
        WorkerInstanceType: !Ref WorkerInstanceType

Outputs:
  WorkerTemplateURL:
    Value: !Sub
      - "https://s3.amazonaws.com/${assetBucket}/${DomainName}/templates/worker.yaml"
      - assetBucket: !Ref assetBucket
        DomainName:  !Ref DomainName
    Export:
      Name: !Sub "${AWS::StackName}-WorkerTemplateURL"
  DomainName:
    Export:
      Name: !Sub "${AWS::StackName}-DomainName"
    Value: !Ref DomainName
  assetBucket:
    Export:
      Name: !Sub "${AWS::StackName}-assetBucket"
    Value: !Ref assetBucket
  ParentZoneID:
    Export:
      Name: !Sub "${AWS::StackName}-ParentZoneID"
    Value: !Ref ParentZoneID
    Condition: hasParentZone
  ControllerSubdomain:
    Export:
      Name: !Sub "${AWS::StackName}-ControllerSubdomain"
    Value: !Ref ControllerSubdomain
  VPCID:
    Export:
      Name: !Sub "${AWS::StackName}-VPCID"
    Value: !Ref VPCID
  PrivateSubnetA:
    Export:
      Name: !Sub "${AWS::StackName}-PrivateSubnetA"
    Value: !Ref PrivateSubnetA
  PrivateSubnetB:
    Export:
      Name: !Sub "${AWS::StackName}-PrivateSubnetB"
    Value: !Ref PrivateSubnetB
  PrivateSubnetC:
    Export:
      Name: !Sub "${AWS::StackName}-PrivateSubnetC"
    Value: !Ref PrivateSubnetC
  PublicSubnetA:
    Export:
      Name: !Sub "${AWS::StackName}-PublicSubnetA"
    Value: !Ref PublicSubnetA
  PublicSubnetB:
    Export:
      Name: !Sub "${AWS::StackName}-PublicSubnetB"
    Value: !Ref PublicSubnetB
  PublicSubnetC:
    Export:
      Name: !Sub "${AWS::StackName}-PublicSubnetC"
    Value: !Ref PublicSubnetC
  HostedZone:
    Export:
      Name: !Sub "${AWS::StackName}-HostedZone"
    Value: !Ref HostedZone
